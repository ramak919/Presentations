# -*- coding: utf-8 -*-
"""Pandas vs SQL OAUG Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qH6KPG2h8jusevHrWiw6uOqgOUDhUXzA
"""

import pandas as pd

"""# Load Sample Data"""

titanic_url='https://raw.githubusercontent.com/ramak919/presentations/master/OAUG/2019%20Pandas%20vs%20SQL/Data/Titanic.csv'
tips_url = 'https://raw.githubusercontent.com/ramak919/presentations/master/OAUG/2019%20Pandas%20vs%20SQL/Data/tips.csv'
employee_url='https://github.com/ramak919/presentations/blob/master/OAUG/2019%20Pandas%20vs%20SQL/Data/Oracle%20Employee%20Data.xlsx?raw=true'

import pandas as pd
import numpy as np

# Read from csv 
titanic_df      = pd.read_csv(titanic_url)
tips_df         = pd.read_csv(tips_url)

# Read Excel with multiple sheets
employee_data   = pd.read_excel(employee_url,sheet_name=None)
employee_df     = employee_data['Employee']
dept_df         = employee_data['Department']


#print(employee_data.keys())
#tips = pd.read_csv(tips_url)

"""# Descriptive Data Info"""

titanic_df.head()

tips_df.shape                    # No of Rows, Columns

tips_df.columns                  # Column  Names

tips_df.describe()               # Statistic Info
#tips_df.describe(include='all')  # Statistical Info including text columns

tips_df.nunique()                # Unique values for each column

tips_df.memory_usage()           # memory Usage by Column

tips_df.info()                   # Column name , no of values and data type

tips_df.shape                    # No of Rows, Columns
tips_df.columns                  # Column  Names
tips_df.describe()               # Statistic Info
tips_df.describe(include='all')  # Statistical Info including text columns
tips_df.nunique()                # Unique values for each column
tips_df.memory_usage()           # memory Usage by Column
tips_df.info()                   # Column name, no of values and data type

"""# Where (Row Selection)"""

tips_df.head(2)                           # Get first 2 rows
tips_df.tail(2)                           # Get Last 2 rows
tips_df.sample(n=5,frac=None)             # 5 random sample of rows
tips_df.sample(n=None,frac=.6)            # Randomly select 60% of rows
tips_df.nlargest(5,columns='total_bill')  # Get 5 Rows with highest fares for a given column
tips_df.nsmallest(5,columns='total_bill') # Get 5 Rows with lowest fares for a given column

tips_df[tips_df['time'] == 'Dinner']         # Get Rows where column time has value Dinner
tips_df[tips_df['day'].str.contains('Th')]   # Like Command  Case sensitive
tips_df[tips_df['day'].isin(['Thur','Fri'])] # IN Command  Case sensitive
tips_df[~tips_df['day'].isin(['Thur','Fri'])]# Not IN Command  Case sensitive

titanic_df[titanic_df['Cabin'].notnull()]   # Rows with value not null in given column
titanic_df[titanic_df['Cabin'].isnull()]    #  Rows with null value in given column
titanic_df.isnull().any()                   # which columns have null values
titanic_df[titanic_df.isnull().any(axis=1)] # Rows with null value in any column 

# tips by parties of at least 5 diners OR bill total was more than $45
tips_df[(tips_df['size'] >= 5) | (tips_df['total_bill'] > 45)]

# tips of more than $5.00 at Dinner meals
tips_df[(tips_df['time'] == 'Dinner') & (tips_df['tip'] > 5.00)]

tips_df.head(2)                           # Get first 2 rows

tips_df.tail(2)                           # Get Last 2 rows

tips_df.sample(n=5,frac=None)             # 5 random sample of rows

tips_df.sample(n=None,frac=.6)            # Randomly select 60% of rows

tips_df.nlargest(5,columns='total_bill')  # Get 5 Rows with highest fares for a given column

tips_df.nsmallest(5,columns='total_bill') # Get 5 Rows with lowest fares for a given column

tips_df[tips_df['time'] == 'Dinner']      # Get Rows where column time has value Dinner

tips_df[tips_df['day'].str.contains('Th')].tail() # Like Command  Case sensitive

#tips_df[tips_df['day'].isin(['Thur','Fri'])]    # IN Command  Case sensitive
tips_df[~tips_df['day'].isin(['Thur','Fri'])]   # Not IN Command  Case sensitive

titanic_df[titanic_df['Cabin'].notnull()]   # Rows with value not null in given column
titanic_df[titanic_df['Cabin'].isnull()]    #  Rows with null value in given column
titanic_df.isnull().any()                   # which columns have null values
titanic_df[titanic_df.isnull().any(axis=1)] # Rows with null value in any column

# tips of more than $5.00 at Dinner meals
tips_df[(tips_df['time'] == 'Dinner') & (tips_df['tip'] > 5.00)]

# tips by parties of at least 5 diners OR bill total was more than $45
tips_df[(tips_df['size'] >= 5) | (tips_df['total_bill'] > 45)]

"""# Joins ( Move Join After Aggregation)"""

#dept_df.head()
employee_df.head()

employee_dept_df=pd.merge(employee_df
                          ,dept_df
                          ,how         ='outer'         # left,right,outer,inner
                          ,on          ='deptno'
                         , indicator   = True)
print(employee_dept_df.shape)

employee_dept_df

#employee_dept_df.tail()

"""# Aggregation ( Group by / Pivot /Cross Tab)

## Pivot
"""

tips_df.pivot_table(  index     = 'day'
                    , columns   = ['time']
                    , values    = ['tip']
                    , aggfunc   = 'mean')

#aggfunc_name='mean'
#aggfunc_name='sum'
#aggfunc_name='max'
aggfunc_name='min'
tips_df.pivot_table(  index     = 'day'
                    , columns   = ['time','sex']
                    , values    = ['tip','total_bill']
                    , aggfunc   = aggfunc_name)

aggfunc_name={'tip'        : np.mean,
              'total_bill' : [min, max, np.mean]}

tips_df.pivot_table( index      = 'day'
                   #, columns   = ['time']
                   , values     = ['tip','total_bill']
                   , aggfunc    = aggfunc_name)



"""## Group by

### Tips Group By
"""

tips_df.groupby('day').agg({  'tip': np.mean
                            , 'day': np.size})

tips_df.groupby('day').agg({  'tip': {"Avg Tip":np.mean}
                            , 'day': {"Count"  :np.size}})

import numpy as np
tips_df.groupby('sex').size()
tips_df.groupby('sex').count()
tips_df.groupby('sex')['total_bill'].count()
tips_df.groupby('day').agg({'tip': np.mean, 'day': np.size})
tips_df.groupby(['smoker', 'day']).agg({'tip': {"tip_size":np.size, "tip_mean":np.mean}})
#tips.groupby(['smoker', 'day']).agg({'tip': {"tip_size":np.size, "tip_mean":np.mean}}).unstack()
tips_df.groupby('day').agg({'tip': np.mean, 'day': np.size})
tips_df.groupby(['smoker', 'day']).agg({'tip': [np.size, np.mean]})

tips_df.groupby('sex').size()

tips_df.groupby('sex').count()

tips_df.groupby('sex')['total_bill'].count()

tips_df.groupby('day').agg({'tip': np.mean, 'day': np.size})

# Filter for rows where group by criteria has less than 5 rows
#(tips.groupby(['smoker', 'day'])[['smoker', 'day','tip']]
# .filter(lambda x: x['tip'].count()<5)
# )

"""### List Agg"""

df_listagg=tips_df[['day','time']].drop_duplicates(subset=['day','time'])
df_listagg

(df_listagg
 .groupby(['time'])['day']
 .apply(', '.join)
 .reset_index())

"""# Union"""

df1 = pd.DataFrame({'city': ['Chicago', 'San Francisco', 'New York City'],
                       'rank': range(1, 4)})
df1

df2 = pd.DataFrame({'city': ['Chicago', 'Boston', 'Los Angeles'],
                     'rank': [1, 4, 5]})
df2

pd.concat([df1, df2])

pd.concat([df1, df2]).drop_duplicates()

"""# Visualization"""

"""‘line’ : line plot (default)
‘bar’ : vertical bar plot
‘barh’ : horizontal bar plot
‘hist’ : histogram
‘box’ : boxplot
‘kde’ : Kernel Density Estimation plot
‘density’ : same as ‘kde’
‘area’ : area plot
‘pie’ : pie plot
‘scatter’ : scatter plot
‘hexbin’ : hexbin plot
"""
#titanic['Pclass'].value_counts().plot()
#employee_dept_df.groupby('dname')['sal'].sum().plot(kind='barh')

#tips

employee_dept_df.groupby('dname',as_index = False)['sal'].sum()

"""## Bar Horizantal"""

employee_dept_df.groupby('dname')['sal'].sum().plot(kind='barh')

"""## Bar Vertical"""

#https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html
#employee_dept_df.groupby('dname')['sal'].sum().plot('bar')
#employee_dept_df.groupby('job')['sal'].sum().plot('bar')

employee_dept_df.groupby(['job','dname'])['sal'].sum().plot(kind ='bar')

"""## Pie"""

employee_dept_df.groupby('job')['sal'].sum().plot(x='job',y='sal',kind='pie')

#employee_dept_df.groupby('dname')['sal'].sum().plot(kind='pie')

"""## Scatter"""

tips_df['per']=tips_df['tip']/tips_df['total_bill'] *100
#tips['per']=np.log(tips['per'])
#tips.plot(kind='scatter',x='total_bill', y='tip',)
#tips.plot.scatter(x='total_bill', y='tip')
#tips.plot.scatter(x='total_bill', y='per',c='size',colormap='viridis',figsize =(20,5)  )
#tips.plot.scatter(x='total_bill', y='tip',c='per',colormap='viridis',figsize =(10,10) )
#tips.plot.scatter(x='total_bill', y='tip',c='size',colormap='viridis',figsize =(20,5) ,s=tips['per'] )

tips_df.plot.scatter( x        = 'total_bill'
                     ,y        = 'tip'
                     ,c        = 'per'
                     ,s        = tips_df['per']*6  # Dot Size
                     ,colormap = 'viridis'
                     ,figsize  = (20,5))


#tips.head(2)

"""## Box Plot"""

#tips[['size','tip']].plot(kind='box')
#tips[['size','tip']].plot.box()

titanic_df[['Age','Fare']].plot(kind='box')

"""## Heat Map"""

#https://blog.algorexhealth.com/2017/09/10-heatmaps-10-python-libraries/
#https://github.com/AlgorexHealth/python-viz-compared/tree/master/data

flights_url     ='https://raw.githubusercontent.com/AlgorexHealth/python-viz-compared/master/data/flights.csv'
flights_df      = pd.read_csv(flights_url)

flight_matrix   = flights_df.pivot("month", "year", "passengers")

flight_matrix

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

fig   = plt.figure(figsize=(12,12))
r     = sns.heatmap(flight_matrix, cmap='BuPu')
r.set_title("Heatmap of Flight Density from 1949 to 1961")

"""# Connection"""

# All of this is basically the same as it would be with Postgres, MySQL, or any other database
# Just pass pandas a connection object and it'll take care of the rest.
from pandas.io import sql
import sqlite3
conn = sqlite3.connect('data/towed.db')
#conn = sqlite3.connect('https://github.com/gjreda/pydata2014nyc/blob/master/data/towed.db?raw=true')

query = "SELECT * FROM towed"
towed = sql.read_sql(query, con=conn, parse_dates={'date':'%m/%d/%Y'})
towed.head()

from pandas_datareader import data
company_lst=[]

#company_lst=['AAPL','GOOG']


stock_df  =      data.DataReader(name         =  company_lst
                                ,start        = '2018'
                                , end         = '2019'
                                , data_source = 'yahoo')

stock_df.head(2)

stock_df.describe()

stock_df['2005':'2006'].describe().head(3)